{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPowerPointLoader\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate,HumanMessagePromptTemplate,ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ppt_data(file_path: str, mode: str = 'elements', verbose: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts content from a PowerPoint file and organizes it by page number.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the PowerPoint file.\n",
    "        mode (str): Mode for loading the PowerPoint file. Default is 'elements'.\n",
    "        verbose (bool): If True, displays progress messages. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are page numbers and values are the concatenated content of each page.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the file path is invalid.\n",
    "        ValueError: If the loader fails to process the file.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"Initializing UnstructuredPowerPointLoader with file: {file_path} and mode: {mode}...\")\n",
    "\n",
    "    try:\n",
    "        loader = UnstructuredPowerPointLoader(file_path=file_path, mode=mode)\n",
    "        docs = loader.load()\n",
    "        if verbose:\n",
    "            print(f\"Successfully loaded {len(docs)} documents.\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"The file '{file_path}' does not exist.\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load PowerPoint file. Error: {str(e)}\")\n",
    "\n",
    "    ppt_data = {}\n",
    "\n",
    "    for idx, doc in enumerate(docs, start=1):\n",
    "        if isinstance(doc, Document):\n",
    "            page_number = doc.metadata.get('page_number', None)\n",
    "            if page_number:\n",
    "                ppt_data[page_number] = ppt_data.get(page_number, '') + '\\n' + doc.page_content\n",
    "            if verbose:\n",
    "                print(f\"Processed document {idx}/{len(docs)}: Page {page_number if page_number else 'break'}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Extraction complete.\")\n",
    "\n",
    "    return ppt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing UnstructuredPowerPointLoader with file: ./ml_course.pptx and mode: elements...\n",
      "Successfully loaded 47 documents.\n",
      "Processed document 1/47: Page 1\n",
      "Processed document 2/47: Page 1\n",
      "Processed document 3/47: Page 1\n",
      "Processed document 4/47: Page 1\n",
      "Processed document 5/47: Page 2\n",
      "Processed document 6/47: Page 2\n",
      "Processed document 7/47: Page 3\n",
      "Processed document 8/47: Page 3\n",
      "Processed document 9/47: Page 3\n",
      "Processed document 10/47: Page 3\n",
      "Processed document 11/47: Page 3\n",
      "Processed document 12/47: Page 3\n",
      "Processed document 13/47: Page 4\n",
      "Processed document 14/47: Page 4\n",
      "Processed document 15/47: Page 4\n",
      "Processed document 16/47: Page 4\n",
      "Processed document 17/47: Page 5\n",
      "Processed document 18/47: Page 5\n",
      "Processed document 19/47: Page 5\n",
      "Processed document 20/47: Page 6\n",
      "Processed document 21/47: Page 6\n",
      "Processed document 22/47: Page 6\n",
      "Processed document 23/47: Page 6\n",
      "Processed document 24/47: Page 6\n",
      "Processed document 25/47: Page 6\n",
      "Processed document 26/47: Page 6\n",
      "Processed document 27/47: Page 6\n",
      "Processed document 28/47: Page 6\n",
      "Processed document 29/47: Page 6\n",
      "Processed document 30/47: Page 6\n",
      "Processed document 31/47: Page 6\n",
      "Processed document 32/47: Page 6\n",
      "Processed document 33/47: Page 6\n",
      "Processed document 34/47: Page 7\n",
      "Processed document 35/47: Page 7\n",
      "Processed document 36/47: Page 7\n",
      "Processed document 37/47: Page 7\n",
      "Processed document 38/47: Page 8\n",
      "Processed document 39/47: Page 8\n",
      "Processed document 40/47: Page 8\n",
      "Processed document 41/47: Page 8\n",
      "Processed document 42/47: Page 8\n",
      "Processed document 43/47: Page 9\n",
      "Processed document 44/47: Page 9\n",
      "Processed document 45/47: Page 9\n",
      "Processed document 46/47: Page 9\n",
      "Processed document 47/47: Page 9\n",
      "Extraction complete.\n"
     ]
    }
   ],
   "source": [
    "ppt_data = extract_ppt_data(file_path=\"./ml_course.pptx\",verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '\\nMachine Learning Model Deployment\\nIntroduction to ML Pipeline\\nhttps://bit.ly/bert_nlp\\n',\n",
       " 2: '\\nWhat is Machine Learning Pipeline?\\n',\n",
       " 3: '\\nType of ML Deployment\\nBatch: In batch deployment, ML models process large volumes of data at scheduled intervals, ideal for tasks like end-of-day reporting or monthly analytics.\\nStream: Stream deployment enables ML models to process and analyze data in real-time as it flows in, suitable for applications like fraud detection or live social media analysis.\\nRealtime: Realtime deployment allows ML models to provide instant predictions or decisions in response to incoming data, essential for use cases like recommendation systems or autonomous driving.\\nEdge: Edge deployment involves running ML models on local devices close to the data source, reducing latency and bandwidth usage, which is crucial for IoT applications and smart devices.\\n',\n",
       " 4: '\\nInfrastructure and Integration\\nHardware and Software: Setting up the right environment for model deployment.\\nIntegration: Seamlessly integrating the model with existing systems and applications.\\n',\n",
       " 5: '\\nBenefits of Deploying ML Models\\nFocus on new models, not maintaining existing models || Prevention of bugs || Creation of records for debugging and reproducing results || Standardization || Allows models to handle real-time data and large user bases.\\n',\n",
       " 6: '\\nChallenges in ML Deployment\\nData Management: Making sure the model gets the right kind of data.\\nModel Scalability and Performance: Ensuring that their model can effectively scale as it keeps adding more complex information.\\nIntegration with Existing Systems: Fitting the model into current computers and software.\\nMonitoring and Maintenance: Watching and fixing the model over time.\\nSecurity and Privacy: Protecting data and keeping it private.\\nResource Management: Using computer resources like memory and power wisely.\\nVersioning and Model Management: Keeping track of different versions of the model.\\nRegulatory Compliance: Making sure the model follows the laws, rules, and regulations.\\nUser Acceptance and Trust: Getting people to trust and accept the model.\\nExplainability and Transparency: Being able to explain how the model works.\\nCost Management: Managing how much it costs to use the model.\\nAs per research, only 13% of ML models ever make it to production. This is a huge gap, considering the possibilities that AI model deployment can bring to the organization.\\n',\n",
       " 7: '\\nData and Model Management\\nData Pipelines: Building and maintaining data pipelines for continuous data flow.\\nModel Versioning: Tracking and managing different versions of models.\\n',\n",
       " 8: '\\nA/B Testing\\nObjective Comparison: A/B testing allows for an objective comparison of two model versions to determine which performs better based on specific metrics.\\nReal-World Application: It is widely used to optimize user experiences, such as testing different recommendation systems or ad strategies to enhance engagement or conversion rates.\\nStatistical Significance: The technique ensures that performance differences are statistically significant and not due to random chance by using control and treatment groups along with statistical tests.\\n',\n",
       " 9: '\\nSecurity, Compliance and Bias\\nSecurity: Ensuring the security of machine learning models involves protecting sensitive data from unauthorized access and breaches through robust encryption, secure APIs, and access controls\\nCompliance: Adhering to industry regulations and standards, such as GDPR or HIPAA, is critical to ensure the legal and ethical use of data in machine learning deployments. This involves data anonymization, user consent, and regular compliance audits.\\nBias Detection: Identifying and mitigating bias in ML models is crucial to prevent unfair and discriminatory outcomes. This involves using diverse training datasets, applying fairness-aware algorithms, and conducting bias impact assessments\\nContinuous Monitoring: Regular monitoring and updating of deployed models are essential to maintain security, compliance, and fairness. This involves real-time performance tracking, automated alerts for anomalies, and periodic model retraining.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context_from_ppt_data(ppt_data: dict, verbose: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Builds a formatted context string from PowerPoint data organized by page number.\n",
    "\n",
    "    Args:\n",
    "        ppt_data (dict): A dictionary where keys are page numbers (int) and \n",
    "                         values are the corresponding page content (str).\n",
    "        verbose (bool): If True, displays progress messages. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted context string with page information.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `ppt_data` is empty or not a dictionary.\n",
    "    \"\"\"\n",
    "    if not isinstance(ppt_data, dict):\n",
    "        raise ValueError(\"Invalid input: `ppt_data` must be a dictionary.\")\n",
    "    if not ppt_data:\n",
    "        raise ValueError(\"Invalid input: `ppt_data` cannot be empty.\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Building context from PowerPoint data...\")\n",
    "\n",
    "    context = \"\"\n",
    "\n",
    "    for page_number, page_content in sorted(ppt_data.items()):\n",
    "        if not isinstance(page_number, int):\n",
    "            if verbose:\n",
    "                print(f\"Skipping invalid page number: {page_number}\")\n",
    "            continue\n",
    "        if not isinstance(page_content, str):\n",
    "            if verbose:\n",
    "                print(f\"Skipping invalid page content for page {page_number}\")\n",
    "            continue\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Adding content for Page-{page_number}...\")\n",
    "\n",
    "        context += f\"### Page-{page_number}\\n{page_content.strip()}\\n\\n\"\n",
    "\n",
    "    if not context:\n",
    "        raise ValueError(\"Context generation failed: No valid content found in `ppt_data`.\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Context generation complete.\")\n",
    "    \n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building context from PowerPoint data...\n",
      "Adding content for Page-1...\n",
      "Adding content for Page-2...\n",
      "Adding content for Page-3...\n",
      "Adding content for Page-4...\n",
      "Adding content for Page-5...\n",
      "Adding content for Page-6...\n",
      "Adding content for Page-7...\n",
      "Adding content for Page-8...\n",
      "Adding content for Page-9...\n",
      "Context generation complete.\n"
     ]
    }
   ],
   "source": [
    "context = build_context_from_ppt_data(ppt_data=ppt_data,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Page-1\n",
      "Machine Learning Model Deployment\n",
      "Introduction to ML Pipeline\n",
      "https://bit.ly/bert_nlp\n",
      "\n",
      "### Page-2\n",
      "What is Machine Learning Pipeline?\n",
      "\n",
      "### Page-3\n",
      "Type of ML Deployment\n",
      "Batch: In batch deployment, ML models process large volumes of data at scheduled intervals, ideal for tasks like end-of-day reporting or monthly analytics.\n",
      "Stream: Stream deployment enables ML models to process and analyze data in real-time as it flows in, suitable for applications like fraud detection or live social media analysis.\n",
      "Realtime: Realtime deployment allows ML models to provide instant predictions or decisions in response to incoming data, essential for use cases like recommendation systems or autonomous driving.\n",
      "Edge: Edge deployment involves running ML models on local devices close to the data source, reducing latency and bandwidth usage, which is crucial for IoT applications and smart devices.\n",
      "\n",
      "### Page-4\n",
      "Infrastructure and Integration\n",
      "Hardware and Software: Setting up the right environment for model deployment.\n",
      "Integration: Seamlessly integrating the model with existing systems and applications.\n",
      "\n",
      "### Page-5\n",
      "Benefits of Deploying ML Models\n",
      "Focus on new models, not maintaining existing models || Prevention of bugs || Creation of records for debugging and reproducing results || Standardization || Allows models to handle real-time data and large user bases.\n",
      "\n",
      "### Page-6\n",
      "Challenges in ML Deployment\n",
      "Data Management: Making sure the model gets the right kind of data.\n",
      "Model Scalability and Performance: Ensuring that their model can effectively scale as it keeps adding more complex information.\n",
      "Integration with Existing Systems: Fitting the model into current computers and software.\n",
      "Monitoring and Maintenance: Watching and fixing the model over time.\n",
      "Security and Privacy: Protecting data and keeping it private.\n",
      "Resource Management: Using computer resources like memory and power wisely.\n",
      "Versioning and Model Management: Keeping track of different versions of the model.\n",
      "Regulatory Compliance: Making sure the model follows the laws, rules, and regulations.\n",
      "User Acceptance and Trust: Getting people to trust and accept the model.\n",
      "Explainability and Transparency: Being able to explain how the model works.\n",
      "Cost Management: Managing how much it costs to use the model.\n",
      "As per research, only 13% of ML models ever make it to production. This is a huge gap, considering the possibilities that AI model deployment can bring to the organization.\n",
      "\n",
      "### Page-7\n",
      "Data and Model Management\n",
      "Data Pipelines: Building and maintaining data pipelines for continuous data flow.\n",
      "Model Versioning: Tracking and managing different versions of models.\n",
      "\n",
      "### Page-8\n",
      "A/B Testing\n",
      "Objective Comparison: A/B testing allows for an objective comparison of two model versions to determine which performs better based on specific metrics.\n",
      "Real-World Application: It is widely used to optimize user experiences, such as testing different recommendation systems or ad strategies to enhance engagement or conversion rates.\n",
      "Statistical Significance: The technique ensures that performance differences are statistically significant and not due to random chance by using control and treatment groups along with statistical tests.\n",
      "\n",
      "### Page-9\n",
      "Security, Compliance and Bias\n",
      "Security: Ensuring the security of machine learning models involves protecting sensitive data from unauthorized access and breaches through robust encryption, secure APIs, and access controls\n",
      "Compliance: Adhering to industry regulations and standards, such as GDPR or HIPAA, is critical to ensure the legal and ethical use of data in machine learning deployments. This involves data anonymization, user consent, and regular compliance audits.\n",
      "Bias Detection: Identifying and mitigating bias in ML models is crucial to prevent unfair and discriminatory outcomes. This involves using diverse training datasets, applying fairness-aware algorithms, and conducting bias impact assessments\n",
      "Continuous Monitoring: Regular monitoring and updating of deployed models are essential to maintain security, compliance, and fairness. This involves real-time performance tracking, automated alerts for anomalies, and periodic model retraining.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = SystemMessagePromptTemplate.from_template(\"\"\"You are helpful AI assistant who create script from the given context \n",
    "                                                          and context is extracted from a PPT.\"\"\")\n",
    "\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\"\"\"Create a script based on the provided context ONLY! If you do not know the answer, just say \"I don't know\".\n",
    "            ### Context:\n",
    "            ```{context}```\n",
    "\n",
    "            ### Question:\n",
    "            ```{question}```\n",
    "\n",
    "            ### Answer:\"\"\")\n",
    "\n",
    "\n",
    "messages = [system_prompt, human_prompt]\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_chain = template | llm | StrOutputParser()\n",
    "\n",
    "def ask_llm(context, question):\n",
    "    return qna_chain.invoke(\n",
    "        {\n",
    "            'context': context,\n",
    "            'question': question\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Slide 1: Introduction to Machine Learning Model Deployment**\n",
      "\n",
      "Welcome everyone to our presentation on Machine Learning Model Deployment. Today, we will dive into the crucial components of the Machine Learning (ML) pipeline, highlighting the processes involved in deploying ML models effectively. For further reading, you can check out the link provided. Let's get started!\n",
      "\n",
      "---\n",
      "\n",
      "**Slide 2: What is Machine Learning Pipeline?**\n",
      "\n",
      "First, let’s clarify what we mean by a Machine Learning pipeline. It is a streamlined series of steps that include data collection, processing, model training, evaluation, and deployment. Each component plays a vital role in ensuring that our models are effective and reliable once they are deployed in real-world applications.\n",
      "\n",
      "---\n",
      "\n",
      "**Slide 3: Types of ML Deployment**\n",
      "\n",
      "Now, let's explore the different types of ML deployment. \n",
      "\n",
      "1. **Batch Deployment** processes large volumes of data at scheduled intervals, making it ideal for tasks like end-of-day reporting or monthly analytics.\n",
      "   \n",
      "2. **Stream Deployment** allows for real-time data processing, perfect for applications like fraud detection or monitoring social media activity as it happens.\n",
      "   \n",
      "3. **Realtime Deployment** provides instant predictions, which is essential for applications such as recommendation systems or autonomous vehicles.\n",
      "   \n",
      "4. Finally, **Edge Deployment** runs models locally on devices close to the data source, greatly reducing latency and bandwidth usage. This is particularly important in IoT applications and smart devices.\n",
      "\n",
      "---\n",
      "\n",
      "**Slide 4: Infrastructure and Integration**\n",
      "\n",
      "Moving on, we must consider the infrastructure and integration required for deploying ML models. This includes selecting the right hardware and software to create a suitable environment for our models. Integration is equally crucial; it’s important to ensure that our models can seamlessly connect with existing systems and applications to function effectively.\n",
      "\n",
      "---\n",
      "\n",
      "**Slide 5: Benefits of Deploying ML Models**\n",
      "\n",
      "There are numerous benefits to deploying ML models. With effective deployment, teams can focus on developing new models rather than maintaining old ones. It can prevent bugs and create records for debugging and reproducing results. Additionally, deploying models standardizes processes, allowing them to handle real-time data and accommodate large user bases efficiently.\n",
      "\n",
      "---\n",
      "\n",
      "**Slide 6: Challenges in ML Deployment**\n",
      "\n",
      "However, deploying ML models comes with its challenges. Data management is critical; we must ensure our models receive the right data. We also need to prioritize model scalability and performance, ensuring they can efficiently scale with increasing complexity. Integration with existing systems often poses hurdles, as do monitoring and maintenance over time. Furthermore, security and privacy are paramount, as well as efficient resource management and versioning. Regulatory compliance, user acceptance, explainability, and cost management must also be considered. Research shows that only 13% of ML models make it to production, indicating a significant gap that exists in the potential of AI deployment.\n",
      "\n",
      "---\n",
      "\n",
      "**Slide 7: Data and Model Management**\n",
      "\n",
      "To facilitate successful deployment, robust data and model management practices are essential. Building and maintaining data pipelines ensures a continuous flow of data. Additionally, model versioning enables tracking and management of different model versions, allowing for easier updates and improvements.\n",
      "\n",
      "---\n",
      "\n",
      "**Slide 8: A/B Testing**\n",
      "\n",
      "A powerful tool in the ML deployment process is A/B testing. This method allows for an objective comparison between two versions of a model, helping us determine which performs better based on specific metrics. A/B testing is particularly useful in real-world applications like optimizing user experiences and testing various strategies to enhance engagement and conversion rates. It ensures that any performance differences we observe are statistically significant, rather than just random fluctuations.\n",
      "\n",
      "---\n",
      "\n",
      "**Slide 9: Security, Compliance and Bias**\n",
      "\n",
      "Lastly, we need to address security, compliance, and bias in our ML deployments. Protecting sensitive data against unauthorized access is vital, and we achieve this through robust encryption and secure APIs. Compliance with regulations, such as GDPR or HIPAA, is crucial for the ethical use of data. Additionally, we must focus on bias detection to prevent any unfair outcomes, utilizing diverse training datasets and fairness-aware algorithms. Continuous monitoring and updating of deployed models are essential to maintain security, compliance, and fairness, ensuring that our models remain effective over time.\n",
      "\n",
      "---\n",
      "\n",
      "Thank you for your attention throughout this presentation. I hope you now have a clearer understanding of the intricacies involved in Machine Learning model deployment.\n"
     ]
    }
   ],
   "source": [
    "question =(\"For each PowerPoint slide provided above, write a 3-4 minute script \"\n",
    "          \"that effectively conveys the key points. Ensure a smooth flow between slides, \"\n",
    "          \"maintaining a clear and engaging narrative.\")\n",
    "\n",
    "response = ask_llm(context=context,\n",
    "                   question=question)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final code with Python OOP's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "from langchain_community.document_loaders import UnstructuredPowerPointLoader\n",
    "from langchain_core.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "from langchain.schema import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerPointProcessor:\n",
    "    \"\"\"\n",
    "    A class to handle PowerPoint content extraction and context generation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the processor.\n",
    "\n",
    "        Args:\n",
    "            verbose (bool): Whether to enable verbose logging.\n",
    "        \"\"\"\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def extract_data(self, file_path: str, mode: str = \"elements\") -> Dict[int, str]:\n",
    "        \"\"\"\n",
    "        Extracts content from a PowerPoint file.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the PowerPoint file.\n",
    "            mode (str): Mode for loading the PowerPoint file. Default is 'elements'.\n",
    "\n",
    "        Returns:\n",
    "            Dict[int, str]: A dictionary where keys are page numbers and values\n",
    "                            are the concatenated content of each page.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the file path is invalid.\n",
    "            ValueError: If the loader fails to process the file.\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"Initializing loader for file: {file_path}, mode: {mode}...\")\n",
    "\n",
    "        try:\n",
    "            loader = UnstructuredPowerPointLoader(file_path=file_path, mode=mode)\n",
    "            docs = loader.load()\n",
    "            if self.verbose:\n",
    "                print(f\"Successfully loaded {len(docs)} documents.\")\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"The file '{file_path}' does not exist.\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to load PowerPoint file. Error: {str(e)}\")\n",
    "\n",
    "        ppt_data = {}\n",
    "        for idx, doc in enumerate(docs, start=1):\n",
    "            if isinstance(doc, Document):\n",
    "                page_number = doc.metadata.get(\"page_number\")\n",
    "                if page_number:\n",
    "                    ppt_data[page_number] = ppt_data.get(page_number, \"\") + \"\\n\" + doc.page_content\n",
    "                if self.verbose:\n",
    "                    print(f\"Processed document {idx}/{len(docs)}: Page {page_number if page_number else 'unknown'}\")\n",
    "\n",
    "        if not ppt_data:\n",
    "            raise ValueError(\"No valid content extracted from the PowerPoint file.\")\n",
    "\n",
    "        return ppt_data\n",
    "\n",
    "    def build_context(self, ppt_data: Dict[int, str]) -> str:\n",
    "        \"\"\"\n",
    "        Builds a formatted context string from PowerPoint data.\n",
    "\n",
    "        Args:\n",
    "            ppt_data (Dict[int, str]): A dictionary where keys are page numbers\n",
    "                                       and values are the corresponding page content.\n",
    "\n",
    "        Returns:\n",
    "            str: A formatted context string with page information.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `ppt_data` is empty or not a dictionary.\n",
    "        \"\"\"\n",
    "        if not isinstance(ppt_data, dict) or not ppt_data:\n",
    "            raise ValueError(\"Invalid `ppt_data`: must be a non-empty dictionary.\")\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Building context from PowerPoint data...\")\n",
    "\n",
    "        context = \"\"\n",
    "        for page_number, page_content in sorted(ppt_data.items()):\n",
    "            if isinstance(page_number, int) and isinstance(page_content, str):\n",
    "                if self.verbose:\n",
    "                    print(f\"Adding content for Page-{page_number}...\")\n",
    "                context += f\"### Page-{page_number}\\n{page_content.strip()}\\n\\n\"\n",
    "            elif self.verbose:\n",
    "                print(f\"Skipping invalid content for Page-{page_number}.\")\n",
    "\n",
    "        if not context.strip():\n",
    "            raise ValueError(\"No valid content to generate context.\")\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Context generation complete.\")\n",
    "\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing loader for file: ./ml_course.pptx, mode: elements...\n",
      "Successfully loaded 47 documents.\n",
      "Processed document 1/47: Page 1\n",
      "Processed document 2/47: Page 1\n",
      "Processed document 3/47: Page 1\n",
      "Processed document 4/47: Page 1\n",
      "Processed document 5/47: Page 2\n",
      "Processed document 6/47: Page 2\n",
      "Processed document 7/47: Page 3\n",
      "Processed document 8/47: Page 3\n",
      "Processed document 9/47: Page 3\n",
      "Processed document 10/47: Page 3\n",
      "Processed document 11/47: Page 3\n",
      "Processed document 12/47: Page 3\n",
      "Processed document 13/47: Page 4\n",
      "Processed document 14/47: Page 4\n",
      "Processed document 15/47: Page 4\n",
      "Processed document 16/47: Page 4\n",
      "Processed document 17/47: Page 5\n",
      "Processed document 18/47: Page 5\n",
      "Processed document 19/47: Page 5\n",
      "Processed document 20/47: Page 6\n",
      "Processed document 21/47: Page 6\n",
      "Processed document 22/47: Page 6\n",
      "Processed document 23/47: Page 6\n",
      "Processed document 24/47: Page 6\n",
      "Processed document 25/47: Page 6\n",
      "Processed document 26/47: Page 6\n",
      "Processed document 27/47: Page 6\n",
      "Processed document 28/47: Page 6\n",
      "Processed document 29/47: Page 6\n",
      "Processed document 30/47: Page 6\n",
      "Processed document 31/47: Page 6\n",
      "Processed document 32/47: Page 6\n",
      "Processed document 33/47: Page 6\n",
      "Processed document 34/47: Page 7\n",
      "Processed document 35/47: Page 7\n",
      "Processed document 36/47: Page 7\n",
      "Processed document 37/47: Page 7\n",
      "Processed document 38/47: Page 8\n",
      "Processed document 39/47: Page 8\n",
      "Processed document 40/47: Page 8\n",
      "Processed document 41/47: Page 8\n",
      "Processed document 42/47: Page 8\n",
      "Processed document 43/47: Page 9\n",
      "Processed document 44/47: Page 9\n",
      "Processed document 45/47: Page 9\n",
      "Processed document 46/47: Page 9\n",
      "Processed document 47/47: Page 9\n",
      "Building context from PowerPoint data...\n",
      "Adding content for Page-1...\n",
      "Adding content for Page-2...\n",
      "Adding content for Page-3...\n",
      "Adding content for Page-4...\n",
      "Adding content for Page-5...\n",
      "Adding content for Page-6...\n",
      "Adding content for Page-7...\n",
      "Adding content for Page-8...\n",
      "Adding content for Page-9...\n",
      "Context generation complete.\n",
      "**Slide 1: Machine Learning Model Deployment - Introduction to ML Pipeline**\n",
      "\n",
      "Welcome, everyone, to our presentation on Machine Learning Model Deployment and the essential concept of the ML Pipeline. Today, we will explore the entire journey of machine learning models, from development to deployment, and discuss how they function in real-world applications. The link provided will take you to a resource that offers deeper insights into Natural Language Processing using BERT, which serves as an excellent example of the capabilities of ML models. Now, let’s dive into the core of our discussion.\n",
      "\n",
      "---\n",
      "\n",
      "**Slide 2: What is Machine Learning Pipeline?**\n",
      "\n",
      "To understand machine learning deployment, we first need to discuss the ML Pipeline itself. Think of the pipeline as the workflow of machine learning, consisting of various stages including data collection, data preprocessing, model training, evaluation, and deployment. Each stage is crucial and must work harmoniously to ensure the model performs effectively. The pipeline not only streamlines the development process but also helps in maintaining consistency and quality throughout. With this foundation laid, let’s move on to the types of ML deployment.\n",
      "\n",
      "---\n",
      "\n",
      "**Slide 3: Types of ML Deployment**\n",
      "\n",
      "Now, let's explore the various types of ML deployments. **Batch deployment** is designed for scenarios where models process large volumes of data at scheduled intervals—think end-of-day reports or monthly analytics. Then we have **stream deployment**, which allows models to analyze data in real-time as it flows in, making it ideal for applications like fraud detection or live social media analysis. Next is **real-time deployment**, which provides instant predictions or decisions essential for use cases such as recommendation systems or autonomous driving. Lastly, **edge deployment** runs models on local devices close to the data source, significantly reducing latency and bandwidth usage—critical for IoT applications. Each type serves unique purposes, and choosing the right one is pivotal for success.\n",
      "\n",
      "---\n",
      "\n",
      "**Slide 4: Infrastructure and Integration**\n",
      "\n",
      "Moving on to the infrastructure and integration aspects. Setting up an effective environment for model deployment requires the right hardware and software components. It's essential to choose systems that can handle the model's requirements. Furthermore, seamless integration with existing systems and applications is crucial. This ensures that your ML model can operate smoothly within the current technological ecosystem, allowing for better data flow and interaction. This brings us to the benefits of deploying ML models.\n",
      "\n",
      "---\n",
      "\n",
      "**Slide 5: Benefits of Deploying ML Models**\n",
      "\n",
      "Deploying machine learning models comes with numerous benefits. First and foremost, it allows teams to focus on developing new models rather than maintaining old ones, thus fostering innovation. Effective deployment can prevent bugs and create valuable records for debugging and reproducing results. It standardizes processes and allows models to handle real-time data efficiently, catering to large user bases. In summary, deploying ML models not only enhances operational efficiency but also paves the way for continuous improvement and innovation.\n",
      "\n",
      "---\n",
      "\n",
      "**Slide 6: Challenges in ML Deployment**\n",
      "\n",
      "However, deploying ML models isn’t without its challenges. One of the primary issues is **data management**—ensuring that the model receives the right kind of data to function optimally. Then there's **model scalability and performance**, which require careful attention as models grow in complexity. Integrating with existing systems can pose hurdles, while ongoing **monitoring and maintenance** are essential to keep the models performing at their best. Additionally, we must consider **security and privacy**, **resource management**, and **compliance with regulations**. Research indicates that only 13% of ML models reach production, highlighting the significant challenges we face in this domain.\n",
      "\n",
      "---\n",
      "\n",
      "**Slide 7: Data and Model Management**\n",
      "\n",
      "To successfully navigate these challenges, effective **data and model management** practices are imperative. By building and maintaining robust **data pipelines**, we can ensure a continuous flow of high-quality data to our models. Moreover, **model versioning** is essential for tracking and managing different versions of models—this helps in maintaining consistency and reliability in our outputs as we iterate and improve. With proper management in place, we can further enhance our deployment strategies.\n",
      "\n",
      "---\n",
      "\n",
      "**Slide 8: A/B Testing**\n",
      "\n",
      "Another vital process in ML deployment is **A/B Testing**. This method facilitates an objective comparison between two model versions, allowing us to determine which one performs better based on specific metrics. In real-world applications, A/B testing can optimize user experiences by testing various recommendation systems or ad strategies, ultimately leading to enhanced engagement or conversion rates. By ensuring that performance differences are statistically significant, we can confidently make data-driven decisions on which model to deploy.\n",
      "\n",
      "---\n",
      "\n",
      "**Slide 9: Security, Compliance, and Bias**\n",
      "\n",
      "Lastly, we must address the critical areas of **security, compliance, and bias**. Protecting sensitive data is paramount; we need robust encryption, secure APIs, and stringent access controls to safeguard our models. Compliance with industry regulations like GDPR or HIPAA is essential for ethical data use, requiring regular audits and data anonymization. Moreover, detecting and mitigating bias in ML models is crucial to prevent unfair outcomes. Implementing diverse training datasets and conducting bias impact assessments can enhance fairness. Continuous monitoring of deployed models is vital to maintain security, compliance, and fairness across all operations.\n",
      "\n",
      "---\n",
      "\n",
      "Thank you for your attention today. By understanding the intricacies of machine learning model deployment and the challenges we face, we can work towards more effective and responsible ML solutions. If you have any questions, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "class ScriptGenerator:\n",
    "    \"\"\"\n",
    "    A class to handle LLM-based script generation from context.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"gpt-4o-mini\", temperature: float = 0.8):\n",
    "        \"\"\"\n",
    "        Initialize the script generator.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the LLM model to use.\n",
    "            temperature (float): Sampling temperature for the model.\n",
    "        \"\"\"\n",
    "        self.llm = ChatOpenAI(model=model_name, temperature=temperature)\n",
    "        self.qna_chain = self._initialize_chain()\n",
    "\n",
    "    def _initialize_chain(self):\n",
    "        \"\"\"\n",
    "        Initialize the LLM chain.\n",
    "\n",
    "        Returns:\n",
    "            Callable: A chain that processes prompts and generates responses.\n",
    "        \"\"\"\n",
    "        system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a highly skilled assistant. Your task is to create a script \"\n",
    "            \"based on the context extracted from a PowerPoint presentation. \"\n",
    "            \"Ensure clarity, engagement, and continuity in the script.\"\n",
    "        )\n",
    "\n",
    "        human_prompt = HumanMessagePromptTemplate.from_template(\n",
    "            \"Based on the context provided below, write a detailed, engaging script for each slide. \"\n",
    "            \"Ensure the narrative flows smoothly and connects the key points.\\n\\n\"\n",
    "            \"### Context:\\n```{context}```\\n\\n### Question:\\n```{question}```\\n\\n### Script:\"\n",
    "        )\n",
    "\n",
    "        template = ChatPromptTemplate.from_messages(messages=[system_prompt, human_prompt])\n",
    "        return template | self.llm | StrOutputParser()\n",
    "\n",
    "    def generate_script(self, context: str, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a script based on the given context and question.\n",
    "\n",
    "        Args:\n",
    "            context (str): The context string.\n",
    "            question (str): The question or instruction for script creation.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated script.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If script generation fails.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.qna_chain.invoke({\"context\": context, \"question\": question})\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Script generation failed. Error: {str(e)}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"./ml_course.pptx\"\n",
    "    verbose = True\n",
    "\n",
    "    processor = PowerPointProcessor(verbose=verbose)\n",
    "    ppt_data = processor.extract_data(file_path=file_path)\n",
    "    context = processor.build_context(ppt_data)\n",
    "\n",
    "    question = (\n",
    "        \"For each PowerPoint slide provided above, write a 3-4 minute script \"\n",
    "        \"that effectively conveys the key points. Ensure a smooth flow between slides, \"\n",
    "        \"maintaining a clear and engaging narrative.\"\n",
    "    )\n",
    "\n",
    "    script_generator = ScriptGenerator()\n",
    "    response = script_generator.generate_script(context=context, question=question)\n",
    "\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "script_generation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
